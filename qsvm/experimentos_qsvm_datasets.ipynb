{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29af5c91",
   "metadata": {},
   "source": [
    "# Experimentos: Baseline com QSVM\n",
    "\n",
    "Este notebook estabelece baselines de performance com QSVM em datasets\n",
    "\n",
    "Usando Seleção de features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e355ce5",
   "metadata": {},
   "source": [
    "# Dataset = Breast Cancer Wisconsin (Diagnostic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29981d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#Ferramentas quânticas \n",
    "from qiskit.circuit.library import ZZFeatureMap\n",
    "from qiskit.primitives import StatevectorSampler    \n",
    "from qiskit_machine_learning.kernels import FidelityQuantumKernel  \n",
    "from qiskit_machine_learning.algorithms import QSVC\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59141e2f",
   "metadata": {},
   "source": [
    "## Loading data and initial analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d80e48ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset carregado com 569 amostras e 30 features.\n"
     ]
    }
   ],
   "source": [
    "# 2. loading data\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "feature_names = load_breast_cancer().feature_names\n",
    "df_X = pd.DataFrame(X, columns=feature_names)\n",
    "\n",
    "print(f\"Dataset carregado com {X.shape[0]} amostras e {X.shape[1]} features.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f993b135",
   "metadata": {},
   "source": [
    "## Split data into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5bbd3fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dados divididos em treino e teste.\n"
     ]
    }
   ],
   "source": [
    "X_train_full, X_test_full, y_train, y_test = train_test_split(\n",
    "    df_X, y, test_size=0.25, random_state=42, stratify=y\n",
    ")\n",
    "print(\"Dados divididos em treino e teste.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e97a432",
   "metadata": {},
   "source": [
    "## Applying feature selection by correlation (30 -> 5 features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b026f938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 features selecionadas: ['worst concave points', 'mean concave points', 'worst perimeter', 'worst radius', 'mean perimeter']\n"
     ]
    }
   ],
   "source": [
    "# Seleção de features com base em correlação (aplicada somente em treino)\n",
    "train_df = X_train_full.copy()\n",
    "train_df[\"diagnosis\"] = y_train\n",
    "\n",
    "# Correlação das features com a variável alvo\n",
    "correlation = train_df.corr(numeric_only=True)[\"diagnosis\"].abs().sort_values(ascending=False)\n",
    "top_features = correlation[1:6].index.tolist()  \n",
    "\n",
    "print(\"Top 5 features selecionadas:\", top_features)\n",
    "\n",
    "# Aplicando seleção nas bases de treino e teste\n",
    "X_train = X_train_full[top_features]\n",
    "X_test = X_test_full[top_features]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d66f6e",
   "metadata": {},
   "source": [
    "## Normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a4104d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dados normalizados.\n"
     ]
    }
   ],
   "source": [
    "# normalizing data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "print(\"Dados normalizados.\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331668d2",
   "metadata": {},
   "source": [
    "# Training QSVM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb2e2c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treinando QSVM...\n",
      "Modelo treinado com sucesso em 573.40 segundos.\n",
      "\n",
      "--- Relatório de Classificação Final (QSVM com seleção de features) ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   malignant       0.80      0.77      0.79        53\n",
      "      benign       0.87      0.89      0.88        90\n",
      "\n",
      "    accuracy                           0.85       143\n",
      "   macro avg       0.84      0.83      0.83       143\n",
      "weighted avg       0.85      0.85      0.85       143\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# criando qsvm\n",
    "num_features = X_train_scaled.shape[1]\n",
    "feature_map = ZZFeatureMap(feature_dimension=num_features, reps=2)\n",
    "fidelity_kernel = FidelityQuantumKernel(feature_map=feature_map)\n",
    "qsvc = QSVC(quantum_kernel=fidelity_kernel, random_state=42)\n",
    "\n",
    "print(\"Treinando QSVM...\")\n",
    "\n",
    "start_time = time.time()\n",
    "qsvc.fit(X_train_scaled, y_train)\n",
    "end_time = time.time()\n",
    "print(f\"Modelo treinado com sucesso em {end_time - start_time:.2f} segundos.\")\n",
    "\n",
    "#  Avaliar\n",
    "predictions = qsvc.predict(X_test_scaled)\n",
    "print(\"\\n--- Relatório de Classificação Final (QSVM com seleção de features) ---\")\n",
    "print(classification_report(y_test, predictions, target_names=load_breast_cancer().target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d910f8",
   "metadata": {},
   "source": [
    "# Dataset = Student Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c6daced",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Importando o dataset\\nstudent_performance = \\'student-por.csv\\'\\n\\ndf_student_performance = pd.read_csv(student_performance, sep=\\',\\')  # Lendo o dataset com separador \\',\\'\\n\\n# 3. Replicamos a separação que a biblioteca fazia para nós:\\n# y são as colunas de notas\\ny = df_student_performance[[\\'G1\\', \\'G2\\', \\'G3\\']]\\n# X são TODAS as outras colunas\\nX = df_student_performance.drop(columns=[\\'G1\\', \\'G2\\', \\'G3\\'])\\n\\n# --- Verificações para confirmar que deu tudo certo ---\\nprint(\"--- Dataset carregado localmente com sucesso! ---\")\\nprint(\"Formato das features (X):\", X.shape)\\nprint(\"Formato dos alvos (y):\", y.shape)\\n\\nprint(\"\\n5 primeiras linhas das features (X):\")\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# Importando o dataset\n",
    "student_performance = 'student-por.csv'\n",
    "\n",
    "df_student_performance = pd.read_csv(student_performance, sep=',')  # Lendo o dataset com separador ','\n",
    "\n",
    "# 3. Replicamos a separação que a biblioteca fazia para nós:\n",
    "# y são as colunas de notas\n",
    "y = df_student_performance[['G1', 'G2', 'G3']]\n",
    "# X são TODAS as outras colunas\n",
    "X = df_student_performance.drop(columns=['G1', 'G2', 'G3'])\n",
    "\n",
    "# --- Verificações para confirmar que deu tudo certo ---\n",
    "print(\"--- Dataset carregado localmente com sucesso! ---\")\n",
    "print(\"Formato das features (X):\", X.shape)\n",
    "print(\"Formato dos alvos (y):\", y.shape)\n",
    "\n",
    "print(\"\\n5 primeiras linhas das features (X):\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a6d0d899",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X.head(5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a5643ed7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Dimensões dos dados do Student Performance Dataset\\nprint(\"\\nDimensões do Student Performance Dataset:\")\\nprint(f\"X: {X.shape}\")\\nprint(f\"y: {y.shape}\")'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# Dimensões dos dados do Student Performance Dataset\n",
    "print(\"\\nDimensões do Student Performance Dataset:\")\n",
    "print(f\"X: {X.shape}\")\n",
    "print(f\"y: {y.shape}\")\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "28a7e860",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Data cleaning\\n# check for missing values\\nprint(\"\\nVerificando valores ausentes:\")\\nprint(X.isnull().sum())'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# Data cleaning\n",
    "# check for missing values\n",
    "print(\"\\nVerificando valores ausentes:\")\n",
    "print(X.isnull().sum())\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7a1564",
   "metadata": {},
   "source": [
    "## Pré-processamento dataset Student Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "594cedbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# creating classification targets\\ny_final = np.where(y[\\'G3\\'] >= 10, 1, 0)  # Convertendo a nota final em uma classificação binária (aprovado/reprovado). Usando 10 como o limite de aprovação.\\n# 1 representa aprovado e 0 representa reprovado.\\n# Verificando a distribuição das classes\\nprint(\"\\nDistribuição das classes (Aprovado/Reprovado):\")\\nprint(pd.Series(y_final).value_counts()) # Contando as ocorrências de cada classe. usando pd.Series para criar uma série pandas a partir do array numpy'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# creating classification targets\n",
    "y_final = np.where(y['G3'] >= 10, 1, 0)  # Convertendo a nota final em uma classificação binária (aprovado/reprovado). Usando 10 como o limite de aprovação.\n",
    "# 1 representa aprovado e 0 representa reprovado.\n",
    "# Verificando a distribuição das classes\n",
    "print(\"\\nDistribuição das classes (Aprovado/Reprovado):\")\n",
    "print(pd.Series(y_final).value_counts()) # Contando as ocorrências de cada classe. usando pd.Series para criar uma série pandas a partir do array numpy\"\"\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "16f3f4c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# The dataset seem unbalanced, so it has to be balanced\\n\\n# tranforming text features into numerical features\\nX_final = pd.get_dummies(X, drop_first=True)  # Convertendo variáveis categóricas em variáveis dummy # drop_first=True evita a armadilha da variável fictícia\\n# Verificando as dimensões dos dados após a codificação\\nprint(\"\\nDimensões dos dados após a codificação:\") \\nprint(\"Formato das features (X) antes do processamento:\", X.shape)\\nprint(\"Formato das features (X) após o processamento:\", X_final.shape)\\n# Verificando as primeiras linhas dos dados codificados\\nprint(\"\\nPrimeiras linhas dos dados codificados:\")\\nprint(X_final.head())\\nprint(X_final.dtypes)'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# The dataset seem unbalanced, so it has to be balanced\n",
    "\n",
    "# tranforming text features into numerical features\n",
    "X_final = pd.get_dummies(X, drop_first=True)  # Convertendo variáveis categóricas em variáveis dummy # drop_first=True evita a armadilha da variável fictícia\n",
    "# Verificando as dimensões dos dados após a codificação\n",
    "print(\"\\nDimensões dos dados após a codificação:\") \n",
    "print(\"Formato das features (X) antes do processamento:\", X.shape)\n",
    "print(\"Formato das features (X) após o processamento:\", X_final.shape)\n",
    "# Verificando as primeiras linhas dos dados codificados\n",
    "print(\"\\nPrimeiras linhas dos dados codificados:\")\n",
    "print(X_final.head())\n",
    "print(X_final.dtypes)\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658bd5ca",
   "metadata": {},
   "source": [
    "## Spliting data into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bfc65197",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'X_train, X_test, y_train, y_test = train_test_split(X_final, y_final, test_size=0.25, random_state=42, stratify=y_final)  # Stratify -> mantém a proporção das classes no split\\n\\nprint(f\"\\nDimensões do conjunto de treino: {X_train.shape}\")\\nprint(f\"Dimensões do conjunto de teste: {X_test.shape}\")'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"X_train, X_test, y_train, y_test = train_test_split(X_final, y_final, test_size=0.25, random_state=42, stratify=y_final)  # Stratify -> mantém a proporção das classes no split\n",
    "\n",
    "print(f\"\\nDimensões do conjunto de treino: {X_train.shape}\")\n",
    "print(f\"Dimensões do conjunto de teste: {X_test.shape}\")\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fb89b6",
   "metadata": {},
   "source": [
    "## Normalizing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d769cbd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'scaler = StandardScaler()\\nX_train_scaled = scaler.fit_transform(X_train)  \\nX_test_scaled = scaler.transform(X_test)  '"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)  \n",
    "X_test_scaled = scaler.transform(X_test)  \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636cba89",
   "metadata": {},
   "source": [
    "## Using SMOTE to balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9cdbe96f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Using SMOTE to balance the dataset\\nfrom imblearn.over_sampling import SMOTE\\n\\nprint(\"Distribuição de classes no treino ANTES do SMOTE:\")\\nprint(pd.Series(y_train).value_counts())\\n\\n# aplying SMOTE\\nsmote = SMOTE(random_state=42)\\nX_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train)\\n\\n# Verificando o balanceamento DEPOIS do SMOTE\\nprint(\"\\nDistribuição de classes no treino DEPOIS do SMOTE:\")\\nprint(pd.Series(y_train_resampled).value_counts())'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# Using SMOTE to balance the dataset\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "print(\"Distribuição de classes no treino ANTES do SMOTE:\")\n",
    "print(pd.Series(y_train).value_counts())\n",
    "\n",
    "# aplying SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# Verificando o balanceamento DEPOIS do SMOTE\n",
    "print(\"\\nDistribuição de classes no treino DEPOIS do SMOTE:\")\n",
    "print(pd.Series(y_train_resampled).value_counts())\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601cadab",
   "metadata": {},
   "source": [
    "## Training the dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tcc_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
